{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# plot_confusion_matrix is a handy visual tool, added in the latest version of scikit-learn\n",
    "# if you are running an older version, comment out this line and just use confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import svm\n",
    "from xgboost import XGBClassifier\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#from imblearn.pipeline import Pipeline\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.read_csv('../data/water_table_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/water_table_training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list= df_target.status_group.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target'] = target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes\n",
    "#31 objs, 8 ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum()\n",
    "#scheme_management         3877\n",
    "#scheme_name              28166\n",
    "#permit                    3056\n",
    "#public_meeting            3334\n",
    "#subvillage                 371\n",
    "#installer                 3655\n",
    "#funder                    3635\n",
    "#consider dropping scheme_name\n",
    "#some kind of connection between funder and installer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target.status_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\"amount_tsh\", \"gps_height\", \"longitude\", \"latitude\", \"num_private\", \"region_code\", \n",
    "                   \"district_code\", \"population\", \"construction_year\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSM Ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = DummyClassifier(strategy=\"most_frequent\")\n",
    "#dummy model that will pick the largest class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df_train[numeric_columns]\n",
    "X = numeric_df.drop(\"num_private\", axis=1)\n",
    "y = df_target[\"status_group\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2021)\n",
    "#just using numeric columns as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(dummy_model, X_train, y_train, cv=3)\n",
    "#score of about 54%\n",
    "#terrible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.suptitle(\"Dummy Model\")\n",
    "\n",
    "plot_confusion_matrix(dummy_model, X_train, y_train, ax=ax, cmap=\"plasma\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next steps:\n",
    "\n",
    "#use missing indicator:\n",
    "#impute:\n",
    "#scheme_management         3877\n",
    "#scheme_name              28166\n",
    "#permit                    3056\n",
    "#public_meeting            3334\n",
    "#subvillage                 371\n",
    "#installer                 3655\n",
    "#funder                    3635\n",
    "\n",
    "#drop:\n",
    "#wpt_name:name of the water point\n",
    "#num_private: isn't given a description\n",
    "#recorded_by: group that recorded data\n",
    "#scheme_name: missing more than half its rows\n",
    "# date recorded\n",
    "\n",
    "#one hot encode all categorical values\n",
    "\n",
    "#scale features\n",
    "\n",
    "# work with either quantity or quantity group\n",
    "#work with  quality group\n",
    "#keep waterpoint_type\n",
    "#keep source class and source \n",
    "#keep payment type\n",
    "#keep management and management group\n",
    "#extraction class and extraction type\n",
    "#get rid of scheme name\n",
    "#get rid of num_private\n",
    "#drop subvillage\n",
    "#drop date_recorded\n",
    "#drop rows for na for funder and installer\n",
    "#drop wpt_name\n",
    "#df_clean = df_train.drop(['quantity'], , axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data cleaning round two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['date_recorded',  \n",
    "'installer',\n",
    "'wpt_name',  \n",
    "'subvillage',  \n",
    "'recorded_by',  \n",
    "'scheme_name',  \n",
    "'extraction_type_group',  \n",
    "'payment',  \n",
    "'water_quality',  \n",
    "'quantity',  \n",
    "'source_type',  \n",
    "'waterpoint_type_group',\n",
    "'num_private',\n",
    "'region',\n",
    "'ward',\n",
    "'id',\n",
    "'public_meeting'           \n",
    "]\n",
    "df_clean = df_train.drop(to_drop, axis = 1)\n",
    "#drop id\n",
    "#drop public meeting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_clean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.shape\n",
    "#(59400, 24)\n",
    "#amount_tsh, gps_height, longitude, latitude, population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = df_clean.columns\n",
    "cat = [ 'funder', 'basin',\n",
    "       'region_code', 'district_code', 'lga',\n",
    "       'scheme_management', 'permit', 'construction_year', 'extraction_type',\n",
    "       'extraction_type_class', 'management', 'management_group',\n",
    "       'payment_type', 'quality_group', 'quantity_group', 'source',\n",
    "       'source_class', 'waterpoint_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.shape\n",
    "#(51329, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# swtich from Ternerary to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.target.value_counts()\n",
    "#functional                 32259\n",
    "#non functional             22824\n",
    "#functional needs repair     4317-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = df_clean.target.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = ['non functional' if i=='functional needs repair' else i for i in target_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['bi_target'] = t_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.drop(['target'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo = df_clean[['region_code', 'scheme_management', 'quality_group', \n",
    "                    'quantity_group', 'source', 'extraction_type_class', 'waterpoint_type', 'bi_target']]\n",
    "df_num = df_clean[['amount_tsh', 'construction_year', 'latitude', 'longitude']]\n",
    "df_completed = df_num.join(df_combo, how='outer')\n",
    "df_completed.shape\n",
    "#(51329, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()\n",
    "df_basin_pop = df_clean[['basin', 'population', 'bi_target']]\n",
    "#subset for just columns being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop_map = df_basin_pop.groupby(['basin']).sum()\n",
    "df_pop_map.head()\n",
    "#groupby to check the total population associated iwth each basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "sns.barplot(x=df_pop_map.index, y=\"population\", data=df_pop_map, ax =ax).set_title('Population Per Basin')\n",
    "#plot of each basin and its population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basin_target = df_basin_pop.drop('population', axis = 1)\n",
    "#drop population from data fram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basin_target_map = df_basin_target.value_counts()\n",
    "#check value_counts() for how many functional or non funcitonal water points each basin has\n",
    "#df_basin_target.groupby([ 'basin']).sum()\n",
    "df_basin_target_map\n",
    "\n",
    "basin_name = ['Pangani', 'Lake Victoria', 'Lake Victoria', 'Rufiji', 'Internal', 'Pangani', 'Wami / Ruvu', 'Lake Tanganyika',\n",
    "              'Wami / Ruvu', 'Lake Tanganyika', 'Internal', 'Lake Nyasa', 'Ruvuma / Southern Coast',\n",
    "              'Rufiji', 'Ruvuma / Southern Coast', 'Lake Rukwa', 'Lake Nyasa', 'Lake Rukwa']\n",
    "\n",
    "target_list = ['functional', 'non functional', 'functional', 'functional', 'functional', 'non functional', 'functional'\n",
    "               , 'non functional', 'non functional', 'functional', 'non functional', 'functional', 'non functional', \n",
    "               'non functional', 'functional', 'non functional', 'non functional', 'functional']\n",
    "\n",
    "count_list = [5160, 4280, 4211, 4033, 3802, 3400, 3116, 3058, 2815, 2792, 2528, 2480, 2462, 2005, 1487, 1440, 1265, 995]\n",
    "\n",
    "dict = {'basin':basin_name, 'target': target_list, 'count': count_list}\n",
    "df_b_t_c = pd.DataFrame(dict)\n",
    "#use information from .value counts to constuct a new dataframe containing\n",
    "#the number of funcitonal or non functional water points for each basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_t_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "sns.barplot(x=\"basin\", y=\"count\", hue=\"target\", data=df_b_t_c, ax = ax).set_title('Functionaility of Waterpoints by Basin')\n",
    "#plot of how many funcitonal and non functional water points each absin has"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folium EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.basin.value_counts()\n",
    "#cheack how many water points each basin has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.bi_target.value_counts()\n",
    "#check target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def namer(name):\n",
    "    \"\"\"takes in a name as a string and returns folium formatted name\"\"\"\n",
    "    named = \"<i>\" + name +\"</i>\"\n",
    "    return named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basin_mapper():\n",
    "    \n",
    "    \"\"\"Takes no argument and plots all 8 basins on folium map\"\"\"\n",
    "    \n",
    "    b_map = folium.Map(location=[-6.1630, 35.7516], zoom_start=6, tiles=\"Stamen Terrain\")\n",
    "    #plots initial space for general area, using tanzania's coordinates\n",
    "    \n",
    "    basins = ['Pangani', 'Lake Victoria', 'Lake Nyasa', 'Lake Rukwa', 'Lake Tanganyika',\n",
    "              'Rufiji', 'Wami / Ruvu', 'Ruvuma / Southern Coast']\n",
    "    \n",
    "    long_lat = [[-5.436390, 38.978951], [-0.755775, 33.438354], [-11.6707, 34.6857], [-7.029620, 31.343060], \n",
    "                [-6.2556, 29.5108], [-7.773888, 39.363889], [-6.11667, 38.81667], [-10.474445, 34.8888]]\n",
    "    \n",
    "    for i in range(0, len(basins)):\n",
    "        folium.Marker(long_lat[i], namer(basins[i]), icon=folium.Icon(color=\"purple\"), tooltip=tooltip).add_to(b_map)\n",
    "        #plots a purple marker for each basin\n",
    "    \n",
    "    return b_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basin_split(bname, df):\n",
    "    \"\"\"takes the name of a basin as a string and the cleaned df. \n",
    "    Returns pair as a list iwth basin name and basin coords.\n",
    "    Returns coords_target as a nested list of waterpoint coords and their functionality tag\n",
    "    \"\"\"\n",
    "    tooltip = \"Click me!\"\n",
    "    basins = ['Pangani', 'Lake Victoria', 'Lake Nyasa', 'Lake Rukwa', 'Lake Tanganyika',\n",
    "          'Rufiji', 'Wami / Ruvu', 'Ruvuma / Southern Coast']\n",
    "    \n",
    "    long_lat = [[-5.436390, 38.978951], [-0.755775, 33.438354], [-11.6707, 34.6857], [-7.029620, 31.343060], \n",
    "                [-6.2556, 29.5108], [-7.773888, 39.363889], [-6.11667, 38.81667], [-10.474445, 34.8888]]\n",
    "    \n",
    "    for name in range(0, len(basins)):\n",
    "        if basins[name] == bname:\n",
    "            pair = [bname, long_lat[name]]\n",
    "            #gets the coordinates for the specifed basin only\n",
    "    \n",
    "    basin_df = df[['latitude', 'longitude', 'basin', 'bi_target']]\n",
    "\n",
    "    df_basin = basin_df.loc[basin_df['basin'] == bname]\n",
    "    \n",
    "    target = df_basin.bi_target.head(200).tolist()\n",
    "    long = df_basin.longitude.head(200).tolist()\n",
    "    lat = df_basin.latitude.head(200).tolist()\n",
    "    \n",
    "    coords = [[la,lo] for la,lo in zip(lat, long)]\n",
    "    #list comp to create lists of water point coordinates, [latitude, longitude]\n",
    "    coords_target = [[t,c] for t,c in zip(target, coords)]\n",
    "    #nested list containing [functionality, [latitude, longitude]]\n",
    "    \n",
    "    #print(coords[:5])\n",
    "\n",
    "    return pair, coords_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_build(basin):\n",
    "    \"\"\"takes in one parameter that consists of a list with a basin name and its coordinates\n",
    "    and the coordinates of every water point connected to that basin. Creates a map with a\n",
    "    marker for the basin and marlers for x amount water points. Builds maps ofindividual basins.\"\"\"\n",
    "    \n",
    "    basin_name = basin[0][0]\n",
    "    basin_coords = basin[0][1]\n",
    "    coords = basin[1]\n",
    "    \n",
    "    b = folium.Map(location= basin_coords, zoom_start=8, tiles=\"Stamen Terrain\")\n",
    "    #plots initial space for general area, using specified basin's coordinates\n",
    "    \n",
    "    tooltip = \"Click me!\"\n",
    "    \n",
    "    for coord in coords:\n",
    "        if coord[0] == 'functional':\n",
    "            folium.Marker(coord[1], \"<i>str(i)</i>\", icon=folium.Icon(color=\"green\"), tooltip=tooltip).add_to(b)\n",
    "        else:\n",
    "            folium.Marker(coord[1], \"<i>str(i)</i>\", icon=folium.Icon(color=\"red\"), tooltip=tooltip).add_to(b)\n",
    "            #plots green markers for functional basin and red markers for non functional basins\n",
    "    \n",
    "    folium.Marker(basin_coords, namer(basin_name), tooltip=tooltip, icon=folium.Icon(color=\"purple\")).add_to(b)\n",
    "    #plots purple ,markers for specified basin\n",
    "        \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = ['Pangani', 'Lake Victoria', 'Lake Nyasa', 'Lake Rukwa', 'Lake Tanganyika',\n",
    "          'Rufiji', 'Wami / Ruvu', 'Ruvuma / Southern Coast']\n",
    "# to use functioon map_build, a basin name must be picked and passed\n",
    "#the return value should be saved in a variable and that variable\n",
    "#must be run in its own cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bname = basins[0]\n",
    "Pangani  = map_build(basin_split(bname, df_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bname_list = bname = basins[1]\n",
    "Lake_Victoria  = map_build(basin_split(bname, df_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bname_list = bname = basins[2]\n",
    "Lake_Nyasa  = map_build(basin_split(bname, df_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bname_list = bname = basins[3]\n",
    "Lake_Rukwa = map_build(basin_split(bname, df_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bname_list = bname = basins[4]\n",
    "Lake_Tanganyika = map_build(basin_split(bname, df_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bname_list = bname = basins[5]\n",
    "Rufigi = map_build(basin_split(bname, df_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bname_list = bname = basins[6]\n",
    "wami_ruvu  = map_build(basin_split(bname, df_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bname_list = bname = basins[7]\n",
    "Ruvuma  = map_build(basin_split(bname, df_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Full_map(df):\n",
    "    \"\"\"Takes in the cleaned data frame and plots all eight basins and however many water points are specified\"\"\"\n",
    "    \n",
    "    basins = ['Pangani', 'Lake Victoria', 'Lake Nyasa', 'Lake Rukwa', 'Lake Tanganyika',\n",
    "          'Rufiji', 'Wami / Ruvu', 'Ruvuma / Southern Coast']\n",
    "    \n",
    "    long_lat = [[-5.436390, 38.978951], [-0.755775, 33.438354], [-11.6707, 34.6857], [-7.029620, 31.343060], \n",
    "                [-6.2556, 29.5108], [-7.773888, 39.363889], [-6.11667, 38.81667], [-10.474445, 34.8888]]\n",
    "    \n",
    "    b = folium.Map(location= [-6.1630, 35.7516], zoom_start=6, tiles=\"Stamen Terrain\")\n",
    "    #plots initial space for general area, using tanzania's coordinates\n",
    "    \n",
    "    tooltip = \"Click me!\"\n",
    "    \n",
    "    for i in range(0, len(basins)):\n",
    "        folium.Marker(long_lat[i], namer(basins[i]), tooltip=tooltip, icon=folium.Icon(color=\"purple\")).add_to(b)\n",
    "        #plots all 8 basins as purple markers\n",
    "\n",
    "        \n",
    "    basin_df = df[['latitude', 'longitude', 'basin', 'bi_target']]\n",
    "\n",
    "    \n",
    "    target = basin_df.bi_target.head(400).tolist()\n",
    "    long = basin_df.longitude.head(400).tolist()\n",
    "    lat = basin_df.latitude.head(400).tolist()\n",
    "    #use of .head() to specify how many waterpoints to ma\n",
    "    \n",
    "            \n",
    "    for k in range(0, len(target)):\n",
    "        if target[i] == 'functional':\n",
    "            #print('here')\n",
    "            folium.Marker([lat[i],long[i]], \"<i>str(i)</i>\", icon=folium.Icon(color=\"green\"), tooltip=tooltip).add_to(b)\n",
    "            #print(trip[0])\n",
    "        else:\n",
    "            print('here')\n",
    "            folium.Marker([lat[i],long[i]], \"<i>str(i)</i>\", icon=folium.Icon(color=\"red\"), tooltip=tooltip).add_to(b)\n",
    "            #plots green markers for functional basin and red markers for non functional basins\n",
    "        \n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Svitlana Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_columns_list = ['amount_tsh', 'longitude', 'latitude', 'construction_year', 'region_code', 'scheme_management', 'quality_group', 'quantity_group', 'source', 'extraction_type_class', 'waterpoint_type']\n",
    "\n",
    "numericals = ['amount_tsh', 'longitude', 'latitude', 'construction_year']\n",
    "categoricals = ['region_code', 'scheme_management', 'quality_group', 'quantity_group', 'source', 'extraction_type_class', 'waterpoint_type']\n",
    "\n",
    "conditions = [df_clean['bi_target'] == 'functional', df_clean['bi_target'] == 'non functional']\n",
    "choices = [0, 1]\n",
    "\n",
    "df_clean['status_no'] = np.select(conditions, choices)\n",
    "df_clean.drop('bi_target', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_clean[final_columns_list]\n",
    "y = df_clean['status_no']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation on the Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_pipe = Pipeline(steps =[('transform', CustomTransformer()),('scaler', StandardScaler()),('logreg', LogisticRegression())])\n",
    "bernoulli_pipe = Pipeline(steps =[('transform', CustomTransformer()),('scaler', StandardScaler()),('bern', BernoulliNB())])\n",
    "knn_pipe = Pipeline(steps =[('transform', CustomTransformer()),('scaler', StandardScaler()),('knn', KNeighborsClassifier())])\n",
    "tree_pipe = Pipeline(steps =[('transform', CustomTransformer()),('scaler', StandardScaler()),('tree', DecisionTreeClassifier())])\n",
    "forest_pipe = Pipeline(steps =[('transform', CustomTransformer()),('scaler', StandardScaler()),('forest', RandomForestClassifier())])\n",
    "xgb_pipe = Pipeline(steps =[('transform', CustomTransformer()),('scaler', StandardScaler()),('xgb', XGBClassifier())])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_params = {\n",
    "   'logreg__C':[.1, .75],\n",
    "   'logreg__max_iter':[500],\n",
    "   'logreg__solver':['lbfgs', 'liblinear']\n",
    "            }\n",
    "                       \n",
    "bernoulli_params = {'bern__alpha': list(np.linspace(.1, 1, 10))}\n",
    "\n",
    "knn_params = {\n",
    "    'knn__n_neighbors': [3, 7, 12], \n",
    "    'knn__weights':['uniform', 'distance'], \n",
    "    'knn__p':[1, 2, 3, 4]\n",
    "             }\n",
    "\n",
    "tree_params = {\n",
    "    'tree__criterion':['gini', 'entropy'],  \n",
    "    'tree__max_depth': [10, 25, 40, 55],\n",
    "    'tree__min_samples_leaf':[1, 2, 3], \n",
    "    'tree__max_features': ['auto', 'sqrt']\n",
    "              } \n",
    "forest_params = {\n",
    "    'forest__n_estimators':[100, 125],\n",
    "    'forest__criterion':['gini', 'entropy'],\n",
    "    'forest__max_depth':[20, 40, None],\n",
    "    'forest__min_samples_leaf':[1, 2, 3], \n",
    "    'forest__max_features': ['auto','sqrt', 'log2']\n",
    "                }\n",
    "\n",
    "xgb_params = {\n",
    "        'xgb__eta':[.7, .9, 1.1],\n",
    "        'xgb__max_depth': [6, 10, 15, 20],\n",
    "        'xgb__learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n",
    "        'xgb__subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'xgb__colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'xgb__colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'xgb__min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n",
    "        'xgb__gamma': [0, 0.25, 0.5, 1.0],\n",
    "        'xgb__reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n",
    "        'xgb__n_estimators': [100, 120]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_grid = GridSearchCV(estimator = logreg_pipe, param_grid = logreg_params, scoring = 'recall')\n",
    "log_grid.fit(X_train, y_train)\n",
    "log_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bern_grid = GridSearchCV(estimator = bernoulli_pipe, param_grid = bernoulli_params, scoring = 'recall')\n",
    "bern_grid.fit(X_train, y_train)\n",
    "bern_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSearchCV on K Nearest Neighbors\n",
    "Tossing this one due to computational weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rs_knn = RandomizedSearchCV(estimator = knn_pipe,\n",
    "#                        param_distributions = knn_params,\n",
    "#                        n_iter = 10,                      \n",
    "#                        random_state=42)\n",
    "\n",
    "#rs_knn.fit(X_train, y_train)\n",
    "#rs_knn.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid = GridSearchCV(estimator = tree_pipe, param_grid = tree_params, scoring = 'recall')\n",
    "tree_grid.fit(X_train, y_train)\n",
    "tree_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSearchCV on Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_forest = RandomizedSearchCV(estimator = forest_pipe,\n",
    "                        param_distributions = forest_params,\n",
    "                        n_iter = 10,                      \n",
    "                        random_state=42)\n",
    "\n",
    "rs_forest.fit(X_train, y_train)\n",
    "rs_forest.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSearchCV on XGB\n",
    "Gridsearch is very computationally heavy so RandomSearch had to be used to reduce computation - still very heavy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RandomizedSearchCV(estimator = xgb_pipe,\n",
    "                        param_distributions = xgb_params,\n",
    "                        n_iter = 10,                      \n",
    "                        random_state=42, return_train_score = True)\n",
    "\n",
    "rs.fit(X_train, y_train)\n",
    "rs.best_params_, rs.best_score_, rs.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_pipe.fit(X_train, y_train)\n",
    "plot_confusion_matrix(logreg_pipe, X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Scores:\", cross_val_score(logreg_pipe, X_train, y_train))\n",
    "print(\"Mean Accuracy:\", cross_val_score(logreg_pipe, X_train, y_train).mean())\n",
    "\"Recall Scores:\", cross_validate(logreg_pipe, X_train, y_train, return_train_score= True, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe.fit(X_train, y_train)\n",
    "plot_confusion_matrix(knn_pipe, X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Accuracy Scores:\", cross_val_score(knn_pipe, X_train, y_train))\n",
    "#\"Recall Scores:\", cross_validate(knn_pipe, X_train, y_train, return_train_score= True, scoring = 'recall')\n",
    "#do not uncomment unless you're comfortable with waiting a really long time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pipe.fit(X_train, y_train)\n",
    "plot_confusion_matrix(tree_pipe, X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Scores:\", cross_val_score(tree_pipe, X_train, y_train))\n",
    "print(\"Mean Accuracy:\", cross_val_score(tree_pipe, X_train, y_train).mean())\n",
    "\"Recall Scores:\", cross_validate(tree_pipe, X_train, y_train, return_train_score= True, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipe.fit(X_train, y_train)\n",
    "plot_confusion_matrix(xgb_pipe, X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Scores:\", cross_val_score(xgb_pipe, X_train, y_train))\n",
    "print(\"Mean Accuracy:\", cross_val_score(xgb_pipe, X_train, y_train).mean())\n",
    "\"Recall Scores:\", cross_validate(xgb_pipe, X_train, y_train, return_train_score= True, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5 - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pipe.fit(X_train, y_train)\n",
    "plot_confusion_matrix(forest_pipe, X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Scores:\", cross_val_score(forest_pipe, X_train, y_train))\n",
    "print(\"Mean Accuracy:\", cross_val_score(forest_pipe, X_train, y_train).mean())\n",
    "\"Recall Scores:\", cross_validate(forest_pipe, X_train, y_train, return_train_score= True, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuned Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned Model 1 - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pipe_tuned = Pipeline(steps =[('transform', CustomTransformer()),('scaler', StandardScaler()), \n",
    "                                   ('tree', DecisionTreeClassifier(criterion = 'entropy',\n",
    "                                                                   max_depth = 30,\n",
    "                                                                   max_features = 'sqrt'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pipe_tuned.fit(X_train, y_train)\n",
    "plot_confusion_matrix(tree_pipe_tuned, X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Scores:\", cross_val_score(tree_pipe_tuned, X_train, y_train))\n",
    "print(\"Mean Accuracy:\", cross_val_score(tree_pipe_tuned, X_train, y_train).mean())\n",
    "\"Recall Scores:\", cross_validate(tree_pipe_tuned, X_train, y_train, return_train_score= True, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned Model 2 - XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipe_tuned = Pipeline(steps =[('transform', CustomTransformer()),('scaler', StandardScaler()), \n",
    "                                  ('xgb', XGBClassifier(subsample=0.9,\n",
    "                                                        reg_lambda=1.0,\n",
    "                                                        n_estimators=150,\n",
    "                                                        min_child_weight=1.0,\n",
    "                                                        max_depth=40,\n",
    "                                                        learning_rate=0.2,\n",
    "                                                        gamma=1.0,\n",
    "                                                        eta=0.9,\n",
    "                                                        colsample_bytree=0.9,\n",
    "                                                        colsample_bylevel=0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipe_tuned.fit(X_train, y_train)\n",
    "plot_confusion_matrix(xgb_pipe_tuned, X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall calculations below take a little while, but they do work.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Scores:\", cross_val_score(xgb_pipe_tuned, X_train, y_train))\n",
    "print(\"Mean Accuracy:\", cross_val_score(xgb_pipe_tuned, X_train, y_train).mean())\n",
    "\"Recall Scores:\", cross_validate(xgb_pipe_tuned, X_train, y_train, return_train_score= True, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example result:\n",
    "\n",
    "Accuracy Scores: [0.78667965 0.79374163 0.78570559 0.78592304 0.78677545]  \n",
    "Mean Accuracy: 0.7877650724093515  \n",
    "('Recall Scores:',  \n",
    " {'fit_time': array([21.27556515, 20.48738503, 20.44357967, 19.84486055, 19.39542603]),  \n",
    "  'score_time': array([0.21004176, 0.20481229, 0.1889286 , 0.17752552, 0.20445323]),  \n",
    "  'test_score': array([0.67903226, 0.70357431, 0.68691212, 0.68494624, 0.6983871 ]),  \n",
    "  'train_score': array([0.74828652, 0.749412  , 0.74826961, 0.7485553 , 0.75251982])})  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned Model 3 - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the search suggested that using gini as a criterion may be a stronger choice, the model seemed to perform better using entropy and with a set maximum depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pipe_tuned = Pipeline(steps =[('transform', CustomTransformer()),('scaler', StandardScaler()),\n",
    "                                     ('forest', RandomForestClassifier(n_estimators=200,\n",
    "                                                                       max_features='sqrt',\n",
    "                                                                       max_depth=45,\n",
    "                                                                       criterion='entropy'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pipe_tuned.fit(X_train, y_train)\n",
    "plot_confusion_matrix(forest_pipe_tuned, X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Scores:\", cross_val_score(forest_pipe_tuned, X_train, y_train))\n",
    "print(\"Mean Accuracy:\", cross_val_score(forest_pipe_tuned, X_train, y_train).mean())\n",
    "\"Recall Scores:\", cross_validate(forest_pipe_tuned, X_train, y_train, return_train_score= True, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classifier predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipe_tuned.fit(X_train, y_train)\n",
    "\n",
    "xgb_train_preds = xgb_pipe_tuned.predict(X_train)\n",
    "xgb_test_preds = xgb_pipe_tuned.predict(X_test)\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_test_preds)\n",
    "xgb_precision = precision_score(y_test, xgb_test_preds)\n",
    "xgb_recall = recall_score(y_test, xgb_test_preds)\n",
    "\n",
    "xgb_y_probas = xgb_pipe_tuned.predict_proba(X_test)[:,1]\n",
    "xgb_tpr, xgb_fpr, xgb_thresholds = roc_curve(y_test, xgb_y_probas)\n",
    "xgb_auc = auc(xgb_tpr, xgb_fpr)\n",
    "\n",
    "\n",
    "print(\"XGBoost Classifier Stats:\")\n",
    "print(\"Test Accuracy: \", xgb_accuracy)\n",
    "print(\"Test Precision: \", xgb_precision)\n",
    "print(\"Test Recall: \", xgb_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pipe_tuned.fit(X_train, y_train)\n",
    "\n",
    "forest_train_preds = forest_pipe_tuned.predict(X_train)\n",
    "forest_test_preds = forest_pipe_tuned.predict(X_test)\n",
    "\n",
    "forest_accuracy = accuracy_score(y_test, forest_test_preds)\n",
    "forest_precision = precision_score(y_test, forest_test_preds)\n",
    "forest_recall = recall_score(y_test, forest_test_preds)\n",
    "\n",
    "forest_y_probas = forest_pipe_tuned.predict_proba(X_test)[:,1]\n",
    "forest_tpr, forest_fpr, forest_thresholds = roc_curve(y_test, forest_y_probas)\n",
    "forest_auc = auc(forest_tpr, forest_fpr)\n",
    "\n",
    "print(\"Random Forest Classifier Stats:\")\n",
    "print(\"Test Accuracy: \", forest_accuracy)\n",
    "print(\"Test Precision: \", forest_precision)\n",
    "print(\"Test Recall: \", forest_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned Model ROC Curves against each other on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.plot(xgb_tpr, xgb_fpr, color = 'b', label = 'XGBoost Classifier')\n",
    "plt.plot(forest_tpr, forest_fpr, color = 'r', label = 'Random Forest Classifier')\n",
    "plt.plot([0, 1], [0, 1], color = 'y', linestyle = '--')\n",
    "\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Tuned Model ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
